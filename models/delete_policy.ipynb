{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress\n",
    "# We want to calculate the union, the intersection, (WMStats - union) and (CMSSW - union)\n",
    "# Then try find if there is a pattern to show us how above's subsets are created\n",
    "\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Do not truncate values\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that putting together all the dataset ids on every day of a week are the same as the\n",
    "# dataset ids of the week\n",
    "def validate_days_in_weeks(week_ts, weeks_ds, days_ds):\n",
    "    \n",
    "    ret_val = True\n",
    "    # Get the set of dataset_ids accessed in the week identified by 'week_ts'\n",
    "    week_set = set(weeks_ds[weeks_ds['week_ts']==week_ts].datasets_set.values[0])    \n",
    "    \n",
    "    # Get the set of dataset_ids accessed in every day that belongs to a week identified by 'week_ts'\n",
    "    days_set=set()\n",
    "    for day_set in days_ds[days_ds['week_ts']==week_ts]['datasets_set']:\n",
    "        days_set.update(set(day_set))\n",
    "    \n",
    "    # Makes sure both sets are the same size\n",
    "    week_set_len= len(week_set)\n",
    "    days_set_len= len(days_set)\n",
    "    if week_set_len != days_set_len:\n",
    "        print(str(week_set_len)+\" != \"+str(days_set_len))\n",
    "        ret_val = False\n",
    "    \n",
    "    # If both sets are the same size proceed to make sure tha both\n",
    "    # sets contain the same items\n",
    "    if ret_val != False:\n",
    "        if days_set != week_set:\n",
    "            ret_val = False\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "# Receives a weeks DataFrame ('weeks_df') and returns a sorted (by week_ts) list\n",
    "# of datasets sets\n",
    "# @weeks_ds: a pandas DataFrame of the form:\n",
    "#    ------------------------------------------------\n",
    "#    |weeks_ts      |datasets_set                   |\n",
    "#    ------------------------------------------------\n",
    "#    |1.561594e+09  |[12686513, 13766504, 14689984] |\n",
    "#    |1.361594e+09  |[15686513, 16766504]           |\n",
    "#    |1.761594e+09  |[17686513, 18766504, 13689984] |\n",
    "#    ------------------------------------------------\n",
    "#    where:\n",
    "#     'weeks_ts' is a Linux timestamp that identifies the week and \n",
    "#     'datasets_set' is an array of datasets IDs that were accessed in that week\n",
    "#    @return: [{15686513, 16766504},{12686513, 13766504, 14689984},{17686513, 18766504, 13689984}] \n",
    "#\n",
    "def get_sorted_list_of_datasets_sets(weeks_df):\n",
    "    # Sort the dataset in cronological order (by week_ts (week timestamp))\n",
    "    # Reset the index once the dataFrame is sorted so that we can access it\n",
    "    # in order using the indices\n",
    "    weeks_df_sorted = weeks_df.sort_values('week_ts')\n",
    "    weeks_df_sorted = weeks_df_sorted.reset_index(drop=True)\n",
    "\n",
    "    # count() returns a series structure, get an integer \n",
    "    weeks_df_count = weeks_df_sorted.count()\n",
    "    weeks_df_count = weeks_df_count.week_ts\n",
    "    # Create a cronological ordered list of datasets sets(arrays are converted into sets)\n",
    "    weeks_sorted_list= []\n",
    "    for i in range(0, weeks_df_count):\n",
    "        weeks_sorted_list.append(set(weeks_df_sorted.datasets_set[i]))\n",
    "        \n",
    "    return weeks_sorted_list\n",
    "\n",
    "def get_freed_recalled_and_ws_sizes(weeks_list, policy, datasets_size):\n",
    "    freed = set()\n",
    "    recalled_per_week = []\n",
    "    freed_per_week = []\n",
    "    called_per_week = []\n",
    "    working_set_size_per_week=[]\n",
    "    ws_per_week = []\n",
    "    to_free = set()\n",
    "    to_recall = set()\n",
    "    \n",
    "    # Fill in the first 'policy' weeks with empty sets given that nothing could have\n",
    "    # recalled nor freed during those weeks.\n",
    "    # The working set size for these first 'policy' weeks will be accumulated set of\n",
    "    # datasets accessed during those weeks\n",
    "    current_working_set = set()\n",
    "    current_working_set_size = 0\n",
    "    for i in range(0, policy):\n",
    "        recalled_per_week.append(to_recall)\n",
    "        freed_per_week.append(to_free)\n",
    "        current_working_set = current_working_set.union(weeks_list[i])\n",
    "        current_working_set_size = get_dataset_set_bytes(current_working_set, datasets_size)\n",
    "        working_set_size_per_week.append(current_working_set_size)\n",
    "   \n",
    "    # For each week in the list, starting on the first week\n",
    "    # after the policy\n",
    "    for i in range(policy, len(weeks_list)):\n",
    "        #print(i)\n",
    "        # Calculate the intermediate working_set that includes the set of datasets\n",
    "        # accesed between the week leaving the working set(old_week) and the\n",
    "        # the current week(new_week)\n",
    "        int_ws = set()\n",
    "        int_ws_to = i - 1\n",
    "        int_ws_from = i - (policy) + 1\n",
    "\n",
    "        #print(\"from: \"+str(int_ws_from))\n",
    "        #print(\"to: \"+str(int_ws_to))\n",
    "        \n",
    "        for j in range(int_ws_from, int_ws_to+1):\n",
    "            #print(\"adding: \"+str(weeks_list[j])+\" to int_ws\")\n",
    "            int_ws.update(weeks_list[j])\n",
    "        new_week = weeks_list[i]\n",
    "        old_week = weeks_list[int_ws_from -1]\n",
    "        #print(old_week)\n",
    "        #print(int_ws)\n",
    "        #print(new_week)\n",
    "        \n",
    "        current_working_set = int_ws.union(new_week)\n",
    "        current_working_set_size = get_dataset_set_bytes(current_working_set, datasets_size)\n",
    "        to_free = old_week - (int_ws.union(new_week))\n",
    "        to_call = (new_week - (int_ws.union(old_week)))\n",
    "        to_recall = (new_week - (int_ws.union(old_week))).intersection(freed)\n",
    "       \n",
    "        working_set_size_per_week.append(current_working_set_size)\n",
    "        freed.update(to_free)\n",
    "        recalled_per_week.append(to_recall)\n",
    "        freed_per_week.append(to_free)\n",
    "        \n",
    "        #called_per_week.append(to_call)\n",
    "        #ws_per_week.append(int_ws.union(old_week))\n",
    "\n",
    "        #print(\"to free: \"+ str(to_free))\n",
    "        #print(\"to call: \"+ str(to_call))\n",
    "        #print(\"to recall: \"+ str(to_recall))\n",
    "\n",
    "    return freed_per_week, recalled_per_week, working_set_size_per_week\n",
    "\n",
    "def get_size_of_datasets_sets(datasets_set, datasets_size):\n",
    "    week_sizes = []\n",
    "    for week in datasets_set:\n",
    "        total_size=0\n",
    "        for dataset_id in week:\n",
    "            size = datasets_size[datasets_size['d_dataset_id'] == dataset_id].dataset_size.values[0]\n",
    "            total_size = total_size + size\n",
    "            #print(\"id: \"+str(dataset_id))\n",
    "            #print(\"size: \"+str(size))\n",
    "        week_sizes.append(total_size)\n",
    "    return week_sizes\n",
    "\n",
    "\n",
    "# Get the set of datasets recalled in every day of a given week\n",
    "def get_datasets_recalled_per_day(recalled_set, week_ts, days_df):\n",
    "    datasets_recalled_per_day = dict()\n",
    "    \n",
    "    for day_ts in days_df[days_df['week_ts'] == week_ts]['day_ts'].values:\n",
    "        datasets_recalled_per_day[day_ts]=set()\n",
    "    \n",
    "    for recalled_dataset in recalled_set:   \n",
    "        # For each of the days in the week\n",
    "        for day_ts in days_df[days_df['week_ts'] == week_ts]['day_ts'].values:\n",
    "            a= days_df['week_ts'] == week_ts\n",
    "            b= days_df['day_ts'] == day_ts\n",
    "            # Is the recalled dataset in this day\n",
    "            if recalled_dataset in days_df[a&b]['datasets_set'].values[0]:\n",
    "                #print(recalled_dataset)\n",
    "                datasets_recalled_per_day[day_ts].add(recalled_dataset)\n",
    "                # If a dataset was accessed more than once within the same week\n",
    "                # it was only recalled the first time since the minimum delete\n",
    "                # policy is 1 week\n",
    "                break\n",
    "                \n",
    "    return datasets_recalled_per_day\n",
    "\n",
    "\n",
    "def get_dataset_set_bytes(datasets_set, datasets_size):\n",
    "    size=0\n",
    "    total_size=0\n",
    "    for dataset_id in datasets_set:\n",
    "        size = datasets_size[datasets_size['d_dataset_id'] == dataset_id].dataset_size.values[0]\n",
    "        total_size = total_size + size\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing dataset ------------------------------------------------------------------\n",
    "days_df = pd.DataFrame(np.array(\n",
    "        [\n",
    "            [1, 1, [1,2,3]], \n",
    "            [1, 2, [1,2]], \n",
    "            [1, 3, [1,5]],\n",
    "            [2, 1, [2,5,6]], \n",
    "            [2, 2, [1]], \n",
    "            [2, 3, [0,9]],\n",
    "            [3, 1, [3]], \n",
    "            [3, 2, [2,4,8]], \n",
    "            [3, 3, [8,0]],\n",
    "            [4, 1, [2]], \n",
    "            [4, 2, [1,0]], \n",
    "            [4, 3, [9,5]],\n",
    "            [5, 1, [1,5,7]], \n",
    "            [5, 2, []], \n",
    "            [5, 3, [1]],\n",
    "        ]\n",
    "        ),columns=['week_ts', 'day_ts', 'datasets_set'])\n",
    "\n",
    "datasets_size = pd.DataFrame(np.array(\n",
    "    [\n",
    "       [0,0], \n",
    "       [1,1], \n",
    "       [2,2],\n",
    "       [3,3], \n",
    "       [4,4], \n",
    "       [5,5], \n",
    "       [6,6], \n",
    "       [7,7], \n",
    "       [8,8], \n",
    "       [9,9], \n",
    "    ]\n",
    "    ), columns=['d_dataset_id','dataset_size'])\n",
    "\n",
    "#datasets_size = pd.DataFrame(np.array(\n",
    "#    [\n",
    "#       [0,11000000], \n",
    "#       [1,14000000], \n",
    "#       [2,9000000],\n",
    "#       [3,18000000], \n",
    "#       [4,1000000], \n",
    "#       [5,4000000], \n",
    "#       [6,9000000], \n",
    "#       [7,15000000], \n",
    "#       [8,20000000], \n",
    "#       [9,11000000], \n",
    "#    ]\n",
    "#    ), columns=['d_dataset_id','dataset_size'])\n",
    "\n",
    "# Calculate 'weeks_ds' out of 'days_ds'\n",
    "weeks_df = days_df.groupby('week_ts').agg({'datasets_set':sum})\n",
    "weeks_df['datasets_set'] = weeks_df['datasets_set'].apply(set)\n",
    "\n",
    "# Shuffle the rows so that they are not sorted by the week_ts as it happens\n",
    "# on the real dataset\n",
    "weeks_df = weeks_df.sample(frac=1)\n",
    "\n",
    "# Insert an index so that 'week_ts' can be accessed as a field nd not as an index\n",
    "weeks_df.reset_index(inplace=True)\n",
    "\n",
    "##---------------------------------------------------------------------------------------\n",
    "datasets_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/dataset.parquet/\")\n",
    "days_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/days_201906.parquet/\")\n",
    "weeks_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/weeks_201906.parquet/\")\n",
    "datasets_size = datasets_df[['d_dataset_id', 'dataset_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561593600.0: True\n",
      "1559174400.0: True\n",
      "1559779200.0: True\n",
      "1560988800.0: True\n",
      "1560384000.0: True\n"
     ]
    }
   ],
   "source": [
    "# Validate that the union of the sets of the days that belong to a week are the same\n",
    "# as the set of the whole week.\n",
    "# Note. This will make no sense if we calculate the 'weeks' dataset from the 'days'\n",
    "for week_ts in weeks_df['week_ts']:\n",
    "    print(str(week_ts) + \": \" + str(validate_days_in_weeks(week_ts, weeks_df, days_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sorted list of the week sets so that the first element in the list\n",
    "# would be the set of dataset IDs accessed in the first week and so on\n",
    "weeks_list = get_sorted_list_of_datasets_sets(weeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max working set size: 1.0491340410239376e+16 Bytes\n"
     ]
    }
   ],
   "source": [
    "# Calculate for each week, the working_set size and the set of datasets freed and recalled depending\n",
    "# on the delete policy used\n",
    "policy=1\n",
    "datasets_freed, datasets_recalled, ws_sizes = get_freed_recalled_and_ws_sizes(weeks_list, policy, datasets_size)\n",
    "max_ws_size = 0\n",
    "for ws_size in ws_sizes:\n",
    "    if ws_size > max_ws_size:\n",
    "        max_ws_size = ws_size\n",
    "        \n",
    "#print(datasets_recalled)\n",
    "print(\"max working set size: \"+str(max_ws_size)+\" Bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the week timestamps sorted so that they corresponds to 'datasets_recalled'\n",
    "weeks_ts = weeks_df.sort_values('week_ts')['week_ts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max recall per day: 455283116162785.0 Bytes, on week: 1560988800.0 on day: 1561420800.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the day with more Bytes recalled\n",
    "max_recall = 0\n",
    "mr_day_ts = 0\n",
    "mr_week_ts = 0\n",
    "for i in range(0, len(weeks_ts)):\n",
    "    #print(weeks_ts[i])\n",
    "    recalled_datasets_per_day= get_datasets_recalled_per_day(datasets_recalled[i], weeks_ts[i], days_df)\n",
    "    #print(recalled_datasets_per_day)\n",
    "    for day in recalled_datasets_per_day:\n",
    "        max_recall_per_week = get_dataset_set_bytes(recalled_datasets_per_day[day], datasets_size)\n",
    "        if max_recall_per_week > max_recall:\n",
    "            max_recall = max_recall_per_week\n",
    "            mr_day_ts = day\n",
    "            mr_week_ts = weeks_ts[i]\n",
    "\n",
    "print(\"max recall per day: \"+str(max_recall)+\" Bytes, on week: \"+str(mr_week_ts)+\" on day: \"+ str(mr_day_ts))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total freed Bytes: 1.0307301195785932e+16\n",
      "total recalled Bytes: 2467554212135155.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the amount of Bytes recalled and freed per week\n",
    "datasets_freed_sizes = get_size_of_datasets_sets(datasets_freed, datasets_size)\n",
    "datasets_recalled_sizes = get_size_of_datasets_sets(datasets_recalled, datasets_size)\n",
    "\n",
    "# Calculate totals\n",
    "total_freed_bytes =0\n",
    "for week in datasets_freed_sizes:\n",
    "    total_freed_bytes = total_freed_bytes + week\n",
    "\n",
    "total_recalled_bytes = 0\n",
    "for week in datasets_recalled_sizes:\n",
    "    total_recalled_bytes = total_recalled_bytes + week\n",
    "    \n",
    "#print(datasets_freed)\n",
    "#print(datasets_freed_sizes)\n",
    "print(\"total freed Bytes: \"+str(total_freed_bytes))\n",
    "#print(datasets_recalled)\n",
    "#print(datasets_recalled_sizes)\n",
    "print(\"total recalled Bytes: \"+str(total_recalled_bytes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
