{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Home made functions\n",
    "from algorithmUtils import *\n",
    "import glob\n",
    "\n",
    "# Do not truncate values\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_record_report(report, policy, max_ws_created_size, max_ws_accessed_size, total_recalled_bytes, total_freed_bytes, \\\n",
    "                      max_recall, mr_week_ts, mr_day_ts, \\\n",
    "                      weeks_ts, ws_size_created_per_week_per_policy, ws_size_accessed_per_week_per_policy, \\\n",
    "                      recalled_size_per_week_per_policy, freed_size_per_week_per_policy, \\\n",
    "                      total_time_s):  \n",
    "    \n",
    "    record = {  'policy': policy,\n",
    "                'max_workingset_created_size':format_bytes(max_ws_created_size),\n",
    "                'max_workingset_accessed_size':format_bytes(max_ws_accessed_size),\n",
    "                'total_recalled':format_bytes(total_recalled_bytes),\n",
    "                'total_freed':format_bytes(total_freed_bytes),\n",
    "                'max_recalled_per_day':format_bytes(max_recall),\n",
    "                'max_recalled_week_ts':mr_week_ts,\n",
    "                'max_recalled_day_ts':mr_day_ts,\n",
    "                'weeks_ts':weeks_ts,\n",
    "                'ws_size_created_per_week_per_policy':ws_size_created_per_week_per_policy,\n",
    "                'ws_size_accessed_per_week_per_policy':ws_size_accessed_per_week_per_policy,\n",
    "                'recalled_size_per_week_per_policy': recalled_size_per_week_per_policy,\n",
    "                'freed_size_per_week_per_policy':freed_size_per_week_per_policy,\n",
    "                'total_time_s':total_time_s,\n",
    "            }\n",
    "    report.append(record)\n",
    "\n",
    "\n",
    "\n",
    "def print_short_report(report):\n",
    "    df = pd.DataFrame(report)\n",
    "    df['max_recalled_week'] = pd.to_datetime(df['max_recalled_week_ts'], unit='s').dt.date\n",
    "    df['max_recalled_day'] = pd.to_datetime(df['max_recalled_day_ts'], unit='s').dt.date\n",
    "    df = df[['policy', 'max_recalled_per_day', 'max_workingset_size','total_recalled', 'total_freed']]\n",
    "    print(df)\n",
    "    \n",
    "def print_full_report(report):\n",
    "    df = pd.DataFrame(report)\n",
    "    df['max_recalled_week'] = pd.to_datetime(df['max_recalled_week_ts'], unit='s').dt.date\n",
    "    df['max_recalled_day'] = pd.to_datetime(df['max_recalled_day_ts'], unit='s').dt.date\n",
    "    #df = df[['policy', 'max_recalled_per_day', 'max_workingset_size','total_recalled', 'total_freed']]\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201904_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201903_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201810_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201902_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201905_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201811_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201809_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201808_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201806_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201901_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201906_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201807_NANO.parquet\n",
      "Reading: /Users/ddavila/projects/DOMA/data/model/datatiers/data_tier_days_201812_NANO.parquet\n"
     ]
    }
   ],
   "source": [
    "basepath=\"/Users/ddavila/projects/DOMA/data/model/\"\n",
    "\n",
    "datasets_size_path = basepath+\"dataset.parquet\"\n",
    "datasets_creation_path = basepath+\"dataset_dates_NANO.parquet\"\n",
    "days_path= basepath+\"datatiers/data_tier_days_*_NANO.parquet\"\n",
    "\n",
    "outputfile=basepath+\"reports/072018_062019_p1_12_NANO-v3.0\"\n",
    "#outputfile=basepath+\"reports/test_NANO\"\n",
    "\n",
    "policy_range = range(1,13)\n",
    "deltaT = 7\n",
    "\n",
    "# REAL data\n",
    "days_df, weeks_df, datasets_creation_df, datasets_size = get_input_data(days_path, datasets_creation_path, datasets_size_path)\n",
    "\n",
    "#TEST data\n",
    "#days_df, weeks_df, datasets_creation_df, datasets_size = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week: 1527724800.0is missing)\n",
      "week: 1540425600.0is missing)\n",
      "week: 1541635200.0is missing)\n",
      "week: 1542844800.0is missing)\n",
      "week: 1544054400.0is missing)\n",
      "week: 1547683200.0is missing)\n",
      "week: 1550102400.0is missing)\n",
      "week: 1551312000.0is missing)\n",
      "week: 1551916800.0is missing)\n",
      "week: 1553126400.0is missing)\n",
      "week: 1554336000.0is missing)\n",
      "week: 1556755200.0is missing)\n",
      "week: 1557964800.0is missing)\n",
      "week: 1558569600.0is missing)\n",
      "week: 1560988800.0is missing at last)\n",
      "week: 1561593600.0is missing at last)\n"
     ]
    }
   ],
   "source": [
    "# Get a list with the timesgtamps of each of the weeks in the time range we are analyzing\n",
    "weeks_ts = weeks_df.sort_values('week_ts')['week_ts'].values\n",
    "start_date = weeks_ts[0]\n",
    "end_date = weeks_ts[len(weeks_ts) - 1]\n",
    "\n",
    "# Get a sorted list of the week sets so that the first element in the list\n",
    "# would be the set of dataset IDs accessed in the first week and so on\n",
    "weeks_list_accesses = get_sorted_list_of_datasets_setsX(weeks_df, \"week_ts\", \"datasets_set\", start_date, end_date)\n",
    "weeks_list_creation = get_sorted_list_of_datasets_setsX(datasets_creation_df, \"creation_week_ts\", \"d_dataset_id\", start_date, end_date)\n",
    "\n",
    "# Make sure that the 3 lists: weeks_ts, weeks_list_accesses and weeks_list_creation have the same length\n",
    "# Otherwise something went wrong\n",
    "if(len(weeks_ts) != len(weeks_list_accesses) or len(weeks_ts) != len(weeks_list_creation)):\n",
    "    print(\"ERROR, lists: weeks_ts, weeks_list_accesses and/or weeks_list_creation have different lengths\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1 took 13.69 s\n",
      "STEP2 took 5.78 s\n",
      "STEP3 took 3.21 s\n",
      "Total time: 22.68 s\n",
      "STEP1 took 13.90 s\n",
      "STEP2 took 3.99 s\n",
      "STEP3 took 2.55 s\n",
      "Total time: 20.44 s\n",
      "STEP1 took 15.82 s\n",
      "STEP2 took 2.92 s\n",
      "STEP3 took 2.00 s\n",
      "Total time: 20.74 s\n",
      "STEP1 took 18.09 s\n",
      "STEP2 took 2.11 s\n",
      "STEP3 took 1.64 s\n",
      "Total time: 21.85 s\n",
      "STEP1 took 18.37 s\n",
      "STEP2 took 1.38 s\n",
      "STEP3 took 1.48 s\n",
      "Total time: 21.24 s\n",
      "STEP1 took 19.97 s\n",
      "STEP2 took 1.25 s\n",
      "STEP3 took 1.33 s\n",
      "Total time: 22.55 s\n",
      "STEP1 took 21.21 s\n",
      "STEP2 took 0.77 s\n",
      "STEP3 took 1.12 s\n",
      "Total time: 23.10 s\n",
      "STEP1 took 21.51 s\n",
      "STEP2 took 0.50 s\n",
      "STEP3 took 1.02 s\n",
      "Total time: 23.03 s\n",
      "STEP1 took 22.05 s\n",
      "STEP2 took 0.46 s\n",
      "STEP3 took 0.99 s\n",
      "Total time: 23.51 s\n",
      "STEP1 took 23.20 s\n",
      "STEP2 took 0.43 s\n",
      "STEP3 took 0.98 s\n",
      "Total time: 24.61 s\n",
      "STEP1 took 24.23 s\n",
      "STEP2 took 0.42 s\n",
      "STEP3 took 0.98 s\n",
      "Total time: 25.63 s\n",
      "STEP1 took 25.07 s\n",
      "STEP2 took 0.38 s\n",
      "STEP3 took 0.93 s\n",
      "Total time: 26.38 s\n"
     ]
    }
   ],
   "source": [
    "report= []\n",
    "#ws_size_created_per_week_per_policy = []\n",
    "#ws_size_accessed_per_week_per_policy = []\n",
    "#recalled_size_per_week_per_policy = []\n",
    "#freed_size_per_week_per_policy = []\n",
    "# Execute general algorithm for each of the different policies\n",
    "for policy in policy_range:\n",
    "    time1 = time.time()\n",
    "\n",
    "    # STEP 1.\n",
    "    # Calculate for each week, the working_set size and the set of datasets freed and recalled depending\n",
    "    # on the delete policy used\n",
    "    datasets_freed, datasets_recalled, working_set_size_created_per_week, working_set_size_accessed_per_week = get_freed_recalled_and_ws_sizes(weeks_list_accesses, weeks_list_creation, policy, policy+deltaT, datasets_size)\n",
    "    #datasets_freed, datasets_recalled, ws_sizes = get_freed_recalled_and_ws_sizes(weeks_list_accesses, weeks_list_creation, policy, policy+deltaT, datasets_size)\n",
    "    # Get the maximum working_set size\n",
    "    max_ws_accessed_size = max(working_set_size_accessed_per_week)\n",
    "    max_ws_created_size =  max(working_set_size_created_per_week)\n",
    "    #ws_size_created_per_week_per_policy.append(to_petabytes(max_ws_created_size))\n",
    "    #ws_size_accessed_per_week_per_policy.append(to_petabytes(max_ws_accessed_size))\n",
    "    time2 = time.time()\n",
    "    print('STEP1 took %0.2f s' % (time2-time1))\n",
    "    \n",
    "\n",
    "    \n",
    "    ## STEP 2.\n",
    "    ## Calculate the day with more Bytes recalled\n",
    "    max_recalled, max_recalled_week_ts, max_recalled_day_ts= get_day_with_max_bytes_recalled(datasets_recalled, weeks_ts,\\\n",
    "                                                                                             days_df, datasets_size) \n",
    "    time3 = time.time()\n",
    "    print('STEP2 took %0.2f s' % (time3-time2))\n",
    "    \n",
    "    ## STEP 3.\n",
    "    recalled_size_per_week, freed_size_per_week, total_recalled, total_freed = get_recalled_and_freed_sizes(datasets_recalled,\\\n",
    "                                                                                                            datasets_freed,\\\n",
    "                                                                                                            datasets_size)\n",
    "    #recalled_size_per_week_per_policy.append(to_petabytes(recalled_size_per_week))\n",
    "    #freed_size_per_week_per_policy.append(to_petabytes(freed_size_per_week))\n",
    "    time4 = time.time()\n",
    "    print('STEP3 took %0.2f s' % (time4-time3))\n",
    "    total_time = (time4-time1)\n",
    "    print('Total time: %0.2f s' % total_time)\n",
    "    \n",
    "    ## Add the result to the report\n",
    "    add_record_report(report, policy, max_ws_created_size, max_ws_accessed_size, total_recalled, total_freed,\\\n",
    "                      max_recalled, max_recalled_week_ts, max_recalled_day_ts,\\\n",
    "                      weeks_ts, working_set_size_created_per_week, working_set_size_accessed_per_week, recalled_size_per_week, freed_size_per_week,\\\n",
    "                      total_time)\n",
    "\n",
    "# Save the report\n",
    "report_df= pd.DataFrame(report)\n",
    "report_df.to_parquet(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    # Build test data\n",
    "    weeks=[\n",
    "        [0,  {0,1}],\n",
    "        [1,  {3,4}],\n",
    "        [2,  {1}],\n",
    "        [3,  {2}],\n",
    "        [4,  {3}],\n",
    "        [5,  {2}],\n",
    "        [6,  {0,1}],\n",
    "        [7,  {4}],\n",
    "       # [8,  {4,3}],\n",
    "       # [9, {4,3}],\n",
    "    ]\n",
    "     \n",
    "    weeks_df = pd.DataFrame(weeks, columns=['week_ts', 'datasets_set'])\n",
    "    days_df =  pd.DataFrame(weeks, columns=['day_ts', 'datasets_set'])\n",
    "    #weeks_df['week_ts_simple'] = df['week_ts']\n",
    "\n",
    "    dataset_size_creation=[\n",
    "        [1,  1,  1000000],\n",
    "        [2,  10,  2000000],\n",
    "        [3,  0,  3000000],\n",
    "        [4,  30,  4000000],\n",
    "        [5,  40,  5000000],\n",
    "        [6,  20,  6000000],\n",
    "        [7,  10,  7000000],\n",
    "        [8,  40,  8000000],\n",
    "        [9,  50,  9000000],\n",
    "        [0, 10, 10000000], \n",
    "    ]\n",
    "    dataset_size_creation_df = pd.DataFrame(dataset_size_creation, columns=['d_dataset_id', 'creation_week_ts','dataset_size'])\n",
    "\n",
    "    datasets_creation_df = dataset_size_creation_df[['creation_week_ts','d_dataset_id']]\n",
    "    datasets_creation_df=datasets_creation_df.groupby('creation_week_ts').agg({'d_dataset_id':list})\n",
    "    datasets_creation_df.reset_index(inplace=True)\n",
    "    datasets_size_df     = dataset_size_creation_df[['d_dataset_id','dataset_size']]\n",
    "\n",
    "    ts_map= {\n",
    "                'week_ts':dict(),\n",
    "                'day_ts':dict(),\n",
    "                'creation_week_ts':dict(),\n",
    "    }\n",
    "\n",
    "    # Thursday Jun 7, 2018 00:00:00 UTC\n",
    "    init_date = 1528329600\n",
    "    seconds_in_week = 3600*24*7\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        ts_map['week_ts'][i]= init_date + (seconds_in_week * i)\n",
    "        ts_map['day_ts'][i]= init_date + (seconds_in_week * i)\n",
    "        ts_map['creation_week_ts'][i]= init_date + (seconds_in_week * i)\n",
    "        \n",
    "    weeks_df=weeks_df.replace(ts_map)    \n",
    "    days_df=days_df.replace(ts_map)    \n",
    "    days_df['week_ts']=days_df['day_ts']\n",
    "\n",
    "    datasets_creation_df=datasets_creation_df.replace(ts_map)\n",
    "    \n",
    "    return days_df, weeks_df, datasets_creation_df, datasets_size_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
