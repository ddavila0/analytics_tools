{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress\n",
    "# We want to calculate the union, the intersection, (WMStats - union) and (CMSSW - union)\n",
    "# Then try find if there is a pattern to show us how above's subsets are created\n",
    "\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Do not truncate values\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that putting together all the dataset ids on every day of a week are the same as the\n",
    "# dataset ids of the week\n",
    "def validate_days_in_weeks(week_ts, weeks_ds, days_ds):\n",
    "    \n",
    "    ret_val = True\n",
    "    # Get the set of dataset_ids accessed in the week identified by 'week_ts'\n",
    "    week_set = set(weeks_ds[weeks_ds['week_ts']==week_ts].datasets_set.values[0])    \n",
    "    \n",
    "    # Get the set of dataset_ids accessed in every day that belongs to a week identified by 'week_ts'\n",
    "    days_set=set()\n",
    "    for day_set in days_ds[days_ds['week_ts']==week_ts]['datasets_set']:\n",
    "        days_set.update(set(day_set))\n",
    "    \n",
    "    # Makes sure both sets are the same size\n",
    "    week_set_len= len(week_set)\n",
    "    days_set_len= len(days_set)\n",
    "    if week_set_len != days_set_len:\n",
    "        print(str(week_set_len)+\" != \"+str(days_set_len))\n",
    "        ret_val = False\n",
    "    \n",
    "    # If both sets are the same size proceed to make sure tha both\n",
    "    # sets contain the same items\n",
    "    if ret_val != False:\n",
    "        if days_set != week_set:\n",
    "            ret_val = False\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "# Receives a weeks DataFrame ('weeks_ds') and returns a sorted (by week_ts) list\n",
    "# of datasets sets\n",
    "# @weeks_ds: a pandas DataFrame of the form:\n",
    "#    ------------------------------------------------\n",
    "#    |weeks_ts      |datasets_set                   |\n",
    "#    ------------------------------------------------\n",
    "#    |1.561594e+09  |[12686513, 13766504, 14689984] |\n",
    "#    |1.361594e+09  |[15686513, 16766504]           |\n",
    "#    |1.761594e+09  |[17686513, 18766504, 13689984] |\n",
    "#    ------------------------------------------------\n",
    "#    where:\n",
    "#     'weeks_ts' is a Linux timestamp that identifies the week and \n",
    "#     'datasets_set' is an array of datasets IDs that were accessed in that week\n",
    "#    @return: [{15686513, 16766504},{12686513, 13766504, 14689984},{17686513, 18766504, 13689984}] \n",
    "#\n",
    "def get_sorted_list_of_datasets_sets(weeks_df):\n",
    "    # Sort the dataset in cronological order (by week_ts (week timestamp))\n",
    "    # Reset the index once the dataFrame is sorted so that we can access it\n",
    "    # in order using the indices\n",
    "    weeks_df_sorted = weeks_df.sort_values('week_ts')\n",
    "    weeks_df_sorted = weeks_df_sorted.reset_index(drop=True)\n",
    "\n",
    "    # count() returns a series structure, get an integer \n",
    "    weeks_df_count = weeks_df_sorted.count()\n",
    "    weeks_df_count = weeks_df_count.week_ts\n",
    "    # Create a cronological ordered list of datasets sets(arrays are converted into sets)\n",
    "    weeks_sorted_list= []\n",
    "    for i in range(0, weeks_df_count):\n",
    "        weeks_sorted_list.append(set(weeks_df_sorted.datasets_set[i]))\n",
    "        \n",
    "    return weeks_sorted_list\n",
    "\n",
    "def get_freed_and_recalled(weeks_list, policy):\n",
    "    freed = set()\n",
    "    recalled = set()\n",
    "    recalled_per_week = dict()\n",
    "    to_free = set()\n",
    "    to_recall = set()\n",
    "    # For each week in the list, starting on the first week\n",
    "    # after the policy\n",
    "    for i in range(policy, len(weeks_list)):\n",
    "        print(i)\n",
    "        # Calculate the current working_set that includes the set of datasets\n",
    "        # accesed within the current week, pointed by 'i', and the previous 'N' \n",
    "        # weeks, where N = 'policy'\n",
    "        working_set = set()\n",
    "        for j in range(i-policy+1, i+1):\n",
    "            #print(\"j: \"+str(j))\n",
    "            working_set.update(weeks_list[j])\n",
    "        new_week = weeks_list[i]\n",
    "        old_week = weeks_list[i-policy]\n",
    "        #print(\"old_week: \"+str(old_week))\n",
    "        #print(\"new_weeks: \"+str(new_weeks))\n",
    "\n",
    "        to_free = old_week - working_set\n",
    "        to_recall = (new_week - old_week).intersection(freed)\n",
    "        #recalled_per_week['']\n",
    "        freed.update(to_free)\n",
    "        recalled.update(to_recall)\n",
    "\n",
    "        print(\"to free: \"+ str(to_free))\n",
    "        print(\"to recall: \"+ str(to_recall))\n",
    "\n",
    "    return len(freed), len(recalled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/dataset.parquet/\")\n",
    "days_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/days_201906.parquet/\")\n",
    "weeks_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/weeks_201906.parquet/\")\n",
    "\n",
    "## Testing dataset ------------------------------------------------------------------\n",
    "my_days_df = pd.DataFrame(np.array(\n",
    "        [\n",
    "            [1, 1, [1,2,3]], \n",
    "            [1, 2, [1,2]], \n",
    "            [1, 3, [1,5]],\n",
    "            [2, 1, [2,5,6]], \n",
    "            [2, 2, [1]], \n",
    "            [2, 3, [0,9]],\n",
    "            [3, 1, [1,3]], \n",
    "            [3, 2, [2,4,8]], \n",
    "            [3, 3, [8,9]],\n",
    "            [4, 1, [2]], \n",
    "            [4, 2, [3,5]], \n",
    "            [4, 3, [9,0]],\n",
    "            [5, 1, [1,5,7]], \n",
    "            [5, 2, []], \n",
    "            [5, 3, [8]],\n",
    "        ]\n",
    "        ),columns=['week_ts', 'day_ts', 'datasets_set'])\n",
    "\n",
    "# Calculate 'weeks_ds' out of 'days_ds'\n",
    "my_weeks_df = my_days_ds.groupby('week_ts').agg({'datasets_set':sum})\n",
    "my_weeks_df['datasets_set'] = my_weeks_ds['datasets_set'].apply(set)\n",
    "\n",
    "# Shuffle the rows so that they are not sorted by the week_ts as it happens\n",
    "# on the real dataset\n",
    "my_weeks_df = my_weeks_df.sample(frac=1)\n",
    "\n",
    "# Insert an index so that 'week_ts' can be accessed as a field nd not as an index\n",
    "my_weeks_df.reset_index(inplace=True)\n",
    "\n",
    "##---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561593600.0: True\n",
      "1559174400.0: True\n",
      "1559779200.0: True\n",
      "1560988800.0: True\n",
      "1560384000.0: True\n",
      "5: True\n",
      "2: True\n",
      "1: True\n",
      "3: True\n",
      "4: True\n"
     ]
    }
   ],
   "source": [
    "# Validate that the union of the sets of the days that belong to a week are the same\n",
    "# as the set of the whole week.\n",
    "# Note. This will make no sense if we calculate the 'weeks' dataset from the 'days' one\n",
    "for week_ts in weeks_df['week_ts']:\n",
    "    print(str(week_ts) + \": \" + str(validate_days_in_weeks(week_ts, weeks_df, days_df)))\n",
    "\n",
    "for my_week_ts in my_weeks_df['week_ts']:\n",
    "    print(str(my_week_ts) + \": \" + str(validate_days_in_weeks(week_ts, weeks_df, days_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_weeks_list = get_sorted_list_of_datasets_sets(my_weeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freed: 9\n",
      "recalled: 5\n",
      "freed: 2\n",
      "recalled: 0\n",
      "freed: 1\n",
      "recalled: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    f,r = get_freed_and_recalled(my_weeks_list, i)\n",
    "    print(\"freed: \"+str(f))\n",
    "    print(\"recalled: \"+str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list=[{1},{2},{0},{1},{3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "to free: {1}\n",
      "to recall: set()\n",
      "3\n",
      "to free: {2}\n",
      "to recall: {1}\n",
      "4\n",
      "to free: {0}\n",
      "to recall: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_freed_and_recalled(my_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
