{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Do not truncate values\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that putting together all the dataset ids on every day of a week are the same as the\n",
    "# dataset ids of the week\n",
    "def validate_days_in_weeks(week_ts, weeks_ds, days_ds):\n",
    "    \n",
    "    ret_val = True\n",
    "    # Get the set of dataset_ids accessed in the week identified by 'week_ts'\n",
    "    week_set = set(weeks_ds[weeks_ds['week_ts']==week_ts].datasets_set.values[0])    \n",
    "    \n",
    "    # Get the set of dataset_ids accessed in every day that belongs to a week identified by 'week_ts'\n",
    "    days_set=set()\n",
    "    for day_set in days_ds[days_ds['week_ts']==week_ts]['datasets_set']:\n",
    "        days_set.update(set(day_set))\n",
    "    \n",
    "    # Makes sure both sets are the same size\n",
    "    week_set_len= len(week_set)\n",
    "    days_set_len= len(days_set)\n",
    "    if week_set_len != days_set_len:\n",
    "        print(str(week_set_len)+\" != \"+str(days_set_len))\n",
    "        ret_val = False\n",
    "    \n",
    "    # If both sets are the same size proceed to make sure tha both\n",
    "    # sets contain the same items\n",
    "    if ret_val != False:\n",
    "        if days_set != week_set:\n",
    "            ret_val = False\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "# Receives a weeks DataFrame ('weeks_df') and returns a sorted (by week_ts) list\n",
    "# of datasets sets\n",
    "# @weeks_ds: a pandas DataFrame of the form:\n",
    "#    ------------------------------------------------\n",
    "#    |weeks_ts      |datasets_set                   |\n",
    "#    ------------------------------------------------\n",
    "#    |1.561594e+09  |[12686513, 13766504, 14689984] |\n",
    "#    |1.361594e+09  |[15686513, 16766504]           |\n",
    "#    |1.761594e+09  |[17686513, 18766504, 13689984] |\n",
    "#    ------------------------------------------------\n",
    "#    where:\n",
    "#     'weeks_ts' is a Linux timestamp that identifies the week and \n",
    "#     'datasets_set' is an array of datasets IDs that were accessed in that week\n",
    "#    @return: [{15686513, 16766504},{12686513, 13766504, 14689984},{17686513, 18766504, 13689984}] \n",
    "#\n",
    "def get_sorted_list_of_datasets_sets(weeks_df):\n",
    "    # Sort the dataset in cronological order (by week_ts (week timestamp))\n",
    "    # Reset the index once the dataFrame is sorted so that we can access it\n",
    "    # in order using the indices\n",
    "    weeks_df_sorted = weeks_df.sort_values('week_ts')\n",
    "    weeks_df_sorted = weeks_df_sorted.reset_index(drop=True)\n",
    "\n",
    "    # count() returns a series structure, get an integer \n",
    "    weeks_df_count = weeks_df_sorted.count()\n",
    "    weeks_df_count = weeks_df_count.week_ts\n",
    "    # Create a cronological ordered list of datasets sets(arrays are converted into sets)\n",
    "    weeks_sorted_list= []\n",
    "    for i in range(0, weeks_df_count):\n",
    "        weeks_sorted_list.append(set(weeks_df_sorted.datasets_set[i]))\n",
    "        \n",
    "    return weeks_sorted_list\n",
    "\n",
    "def get_freed_recalled_and_ws_sizes(weeks_list, policy, datasets_size):\n",
    "    freed = set()\n",
    "    recalled_per_week = []\n",
    "    freed_per_week = []\n",
    "    called_per_week = []\n",
    "    working_set_size_per_week=[]\n",
    "    ws_per_week = []\n",
    "    to_free = set()\n",
    "    to_recall = set()\n",
    "    \n",
    "    # Fill in the first 'policy' weeks with empty sets given that nothing could have\n",
    "    # recalled nor freed during those weeks.\n",
    "    # The working set size for these first 'policy' weeks will be accumulated set of\n",
    "    # datasets accessed during those weeks\n",
    "    current_working_set = set()\n",
    "    current_working_set_size = 0\n",
    "    for i in range(0, policy):\n",
    "        recalled_per_week.append(to_recall)\n",
    "        freed_per_week.append(to_free)\n",
    "        current_working_set = current_working_set.union(weeks_list[i])\n",
    "        current_working_set_size = get_dataset_set_bytes(current_working_set, datasets_size)\n",
    "        working_set_size_per_week.append(current_working_set_size)\n",
    "   \n",
    "    # For each week in the list, starting on the first week\n",
    "    # after the policy\n",
    "    for i in range(policy, len(weeks_list)):\n",
    "        #print(i)\n",
    "        # Calculate the intermediate working_set that includes the set of datasets\n",
    "        # accesed between the week leaving the working set(old_week) and the\n",
    "        # the current week(new_week)\n",
    "        int_ws = set()\n",
    "        int_ws_to = i - 1\n",
    "        int_ws_from = i - (policy) + 1\n",
    "\n",
    "        #print(\"from: \"+str(int_ws_from))\n",
    "        #print(\"to: \"+str(int_ws_to))\n",
    "        \n",
    "        for j in range(int_ws_from, int_ws_to+1):\n",
    "            #print(\"adding: \"+str(weeks_list[j])+\" to int_ws\")\n",
    "            int_ws.update(weeks_list[j])\n",
    "        new_week = weeks_list[i]\n",
    "        old_week = weeks_list[int_ws_from -1]\n",
    "        #print(old_week)\n",
    "        #print(int_ws)\n",
    "        #print(new_week)\n",
    "        \n",
    "        current_working_set = int_ws.union(new_week)\n",
    "        current_working_set_size = get_dataset_set_bytes(current_working_set, datasets_size)\n",
    "        to_free = old_week - (int_ws.union(new_week))\n",
    "        to_call = (new_week - (int_ws.union(old_week)))\n",
    "        to_recall = (new_week - (int_ws.union(old_week))).intersection(freed)\n",
    "       \n",
    "        working_set_size_per_week.append(current_working_set_size)\n",
    "        freed.update(to_free)\n",
    "        recalled_per_week.append(to_recall)\n",
    "        freed_per_week.append(to_free)\n",
    "        \n",
    "        #called_per_week.append(to_call)\n",
    "        #ws_per_week.append(int_ws.union(old_week))\n",
    "\n",
    "        #print(\"to free: \"+ str(to_free))\n",
    "        #print(\"to call: \"+ str(to_call))\n",
    "        #print(\"to recall: \"+ str(to_recall))\n",
    "\n",
    "    return freed_per_week, recalled_per_week, working_set_size_per_week\n",
    "\n",
    "def get_size_of_datasets_sets(datasets_set, datasets_size):\n",
    "    # TODO:\n",
    "    # - call get_dataset_set_bytes\n",
    "    week_sizes = []\n",
    "    for week in datasets_set:\n",
    "        total_size=0\n",
    "        for dataset_id in week:\n",
    "            size = datasets_size[datasets_size['d_dataset_id'] == dataset_id].dataset_size.values[0]\n",
    "            total_size = total_size + size\n",
    "            #print(\"id: \"+str(dataset_id))\n",
    "            #print(\"size: \"+str(size))\n",
    "        week_sizes.append(total_size)\n",
    "    return week_sizes\n",
    "\n",
    "\n",
    "# Get the set of datasets recalled in every day of a given week\n",
    "def get_datasets_recalled_per_day(recalled_set, week_ts, days_df):\n",
    "    datasets_recalled_per_day = dict()\n",
    "    \n",
    "    for day_ts in days_df[days_df['week_ts'] == week_ts]['day_ts'].values:\n",
    "        datasets_recalled_per_day[day_ts]=set()\n",
    "    \n",
    "    for recalled_dataset in recalled_set:   \n",
    "        # For each of the days in the week\n",
    "        for day_ts in days_df[days_df['week_ts'] == week_ts]['day_ts'].values:\n",
    "            a= days_df['week_ts'] == week_ts\n",
    "            b= days_df['day_ts'] == day_ts\n",
    "            # Is the recalled dataset in this day\n",
    "            if recalled_dataset in days_df[a&b]['datasets_set'].values[0]:\n",
    "                #print(recalled_dataset)\n",
    "                datasets_recalled_per_day[day_ts].add(recalled_dataset)\n",
    "                # If a dataset was accessed more than once within the same week\n",
    "                # it was only recalled the first time since the minimum delete\n",
    "                # policy is 1 week\n",
    "                break\n",
    "                \n",
    "    return datasets_recalled_per_day\n",
    "\n",
    "\n",
    "def get_dataset_set_bytes(datasets_set, datasets_size):\n",
    "    size=0\n",
    "    total_size=0\n",
    "    for dataset_id in datasets_set:\n",
    "        size = datasets_size[datasets_size['d_dataset_id'] == dataset_id].dataset_size.values[0]\n",
    "        total_size = total_size + size\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "def read_and_prepare_data(test=False):\n",
    "    days_df_list = [] \n",
    "    if(test):\n",
    "        for i in range(1, 3):\n",
    "            inputfile = \"/Users/ddavila/projects/DOMA/data/model/df\"+str(i)+\".parquet\"\n",
    "            #print(\"reading: \"+inputfile)\n",
    "            day_df = pd.read_parquet(inputfile)\n",
    "            days_df_list.append(day_df)\n",
    "    else:\n",
    "        for i in range(1, 7):\n",
    "            inputfile = \"/Users/ddavila/projects/DOMA/data/model/days_20190\"+str(i)+\".parquet\"\n",
    "            print(\"reading: \"+inputfile)\n",
    "            day_df = pd.read_parquet(inputfile)\n",
    "            days_df_list.append(day_df)\n",
    "    \n",
    "    # Data is coming separated in months, let's concatenate all these months\n",
    "    # to make a single DataFrame containing all the data\n",
    "    all_days = pd.concat(days_df_list)\n",
    "    all_days.reset_index(inplace=True)\n",
    "    \n",
    "    # Transform 'datasets_set' from array to list, so that we can \n",
    "    all_days['datasets_set']=all_days['datasets_set'].apply(list)\n",
    "    \n",
    "    # Make sure that the same week_ts + day_ts doesn't exist in more than 1 month\n",
    "    # and if so, group it together\n",
    "    all_days= all_days.groupby(['week_ts','day_ts']).agg({'datasets_set':sum})\n",
    "    all_days.reset_index(inplace=True)\n",
    "    \n",
    "    # Create a new DataFrame 'all_weeks' where we group all days, within a week, together\n",
    "    all_weeks = all_days.groupby(['week_ts']).agg({'datasets_set':sum})\n",
    "\n",
    "    # Transform the 'datasets_set' from list to set, to remove duplicates\n",
    "    all_weeks['datasets_set']=all_weeks['datasets_set'].apply(set)\n",
    "    all_weeks.reset_index(inplace=True)\n",
    "    \n",
    "    # Transform the 'datasets_set' from list to set, to remove duplicates\n",
    "    all_days['datasets_set']=all_days['datasets_set'].apply(set)\n",
    "    all_days.reset_index(inplace=True)\n",
    "\n",
    "    return all_days, all_weeks\n",
    "\n",
    "def add_record_report(report, policy, max_ws_size, total_recalled_bytes, total_freed_bytes, max_recall, mr_week_ts, mr_day_ts):\n",
    "    record = { 'policy': policy,\n",
    "                'max_workingset_size':format_bytes(max_ws_size),\n",
    "                'total_recalled':format_bytes(total_recalled_bytes),\n",
    "                'total_freed':format_bytes(total_freed_bytes),\n",
    "                'max_recalled_per_day':format_bytes(max_recall),\n",
    "                'max_recalled_week_ts':mr_week_ts,\n",
    "                'max_recalled_day_ts':mr_day_ts,\n",
    "            }\n",
    "    #print(record)\n",
    "    report.append(record)\n",
    "    #print(report)\n",
    "\n",
    "\n",
    "\n",
    "def print_report(report):\n",
    "    df = pd.DataFrame(report)\n",
    "    df['max_recalled_week'] = pd.to_datetime(df['max_recalled_week_ts'], unit='s').dt.date\n",
    "    df['max_recalled_day'] = pd.to_datetime(df['max_recalled_day_ts'], unit='s').dt.date\n",
    "    df = df[['policy', 'max_recalled_per_day', 'max_workingset_size','total_recalled', 'total_freed']]\n",
    "    print(df)\n",
    "\n",
    "def format_bytes(size):\n",
    "    # 2**10 = 1024\n",
    "    #power = 2**10\n",
    "    power = 1000\n",
    "    n = 0\n",
    "    power_labels = {0 : 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB', 5:'PB'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    formated = \"{0:.2f}\".format(size)\n",
    "    formated = formated +\" \"+power_labels[n]\n",
    "    return formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201901.parquet\n",
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201902.parquet\n",
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201903.parquet\n",
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201904.parquet\n",
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201905.parquet\n",
      "reading: /Users/ddavila/projects/DOMA/data/model/days_201906.parquet\n"
     ]
    }
   ],
   "source": [
    "# Use this for REAL data\n",
    "datasets_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/dataset.parquet/\")\n",
    "datasets_size = datasets_df[['d_dataset_id', 'dataset_size']]\n",
    "days_df, weeks_df = read_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for TEST data\n",
    "datasets_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/dfs.parquet/\")\n",
    "datasets_size = datasets_df[['d_dataset_id', 'dataset_size']]\n",
    "days_df, weeks_df = read_and_prepare_data(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the union of the sets of the days that belong to a week are the same\n",
    "# as the set of the whole week.\n",
    "# Note. This will make no sense if we calculate the 'weeks' dataset from the 'days'\n",
    "#for week_ts in weeks_df['week_ts']:\n",
    "#    print(str(week_ts) + \": \" + str(validate_days_in_weeks(week_ts, weeks_df, days_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sorted list of the week sets so that the first element in the list\n",
    "# would be the set of dataset IDs accessed in the first week and so on\n",
    "weeks_list = get_sorted_list_of_datasets_sets(weeks_df)\n",
    "\n",
    "report= []\n",
    "# Execute general algorithm for each of the different policies\n",
    "for policy in range(3,6):\n",
    "   \n",
    "    # Calculate for each week, the working_set size and the set of datasets freed and recalled depending\n",
    "    # on the delete policy used\n",
    "    datasets_freed, datasets_recalled, ws_sizes = get_freed_recalled_and_ws_sizes(weeks_list, policy, datasets_size)\n",
    "    max_ws_size = 0\n",
    "    for ws_size in ws_sizes:\n",
    "        if ws_size > max_ws_size:\n",
    "            max_ws_size = ws_size\n",
    "\n",
    "    #print(datasets_recalled)\n",
    "    #print(\"max working set size: \"+str(max_ws_size)+\" Bytes\")\n",
    "\n",
    "    # Get the week timestamps sorted so that they corresponds to 'datasets_recalled'\n",
    "    weeks_ts = weeks_df.sort_values('week_ts')['week_ts'].values\n",
    "    #print(weeks_ts)\n",
    "\n",
    "    # Calculate the day with more Bytes recalled\n",
    "    max_recall = 0\n",
    "    mr_day_ts = 0\n",
    "    mr_week_ts = 0\n",
    "    for i in range(0, len(weeks_ts)):\n",
    "        #print(weeks_ts[i])\n",
    "        recalled_datasets_per_day= get_datasets_recalled_per_day(datasets_recalled[i], weeks_ts[i], days_df)\n",
    "        #print(recalled_datasets_per_day)\n",
    "        for day in recalled_datasets_per_day:\n",
    "            max_recall_per_week = get_dataset_set_bytes(recalled_datasets_per_day[day], datasets_size)\n",
    "            if max_recall_per_week > max_recall:\n",
    "                max_recall = max_recall_per_week\n",
    "                mr_day_ts = day\n",
    "                mr_week_ts = weeks_ts[i]\n",
    "\n",
    "    #print(\"max recall per day: \"+str(max_recall)+\" Bytes, on week: \"+str(mr_week_ts)+\" on day: \"+ str(mr_day_ts))\n",
    "\n",
    "\n",
    "    # Calculate the amount of Bytes recalled and freed per week\n",
    "    datasets_freed_sizes = get_size_of_datasets_sets(datasets_freed, datasets_size)\n",
    "    datasets_recalled_sizes = get_size_of_datasets_sets(datasets_recalled, datasets_size)\n",
    "\n",
    "    # Calculate totals\n",
    "    total_freed_bytes =0\n",
    "    for week in datasets_freed_sizes:\n",
    "        total_freed_bytes = total_freed_bytes + week\n",
    "\n",
    "    total_recalled_bytes = 0\n",
    "    for week in datasets_recalled_sizes:\n",
    "        total_recalled_bytes = total_recalled_bytes + week\n",
    "\n",
    "    add_record_report(report, policy, max_ws_size, total_recalled_bytes, total_freed_bytes, max_recall, mr_week_ts, mr_day_ts)\n",
    "\n",
    "    #print(datasets_freed)\n",
    "    #print(datasets_freed_sizes)\n",
    "    #print(\"total freed Bytes: \"+str(total_freed_bytes))\n",
    "    #print(datasets_recalled)\n",
    "    #print(datasets_recalled_sizes)\n",
    "    #print(\"total recalled Bytes: \"+str(total_recalled_bytes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   policy max_recalled_per_day max_workingset_size total_recalled total_freed\n",
      "0  3       670.78 TB            18.87 PB            17.32 PB       41.61 PB  \n",
      "1  4       615.54 TB            20.07 PB            12.31 PB       35.46 PB  \n",
      "2  5       590.30 TB            21.40 PB            9.40 PB        30.97 PB  \n"
     ]
    }
   ],
   "source": [
    "print_report(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   policy max_recalled_per_day max_workingset_size total_recalled total_freed\n",
      "0  1       1.42 PB              14.75 PB            40.30 PB       70.37 PB  \n",
      "1  2       812.02 TB            17.22 PB            25.37 PB       52.40 PB  \n"
     ]
    }
   ],
   "source": [
    "print_report(old_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report= old_report + report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   policy max_recalled_per_day max_workingset_size total_recalled total_freed\n",
      "0  1       1.42 PB              14.75 PB            40.30 PB       70.37 PB  \n",
      "1  2       812.02 TB            17.22 PB            25.37 PB       52.40 PB  \n",
      "2  3       670.78 TB            18.87 PB            17.32 PB       41.61 PB  \n",
      "3  4       615.54 TB            20.07 PB            12.31 PB       35.46 PB  \n",
      "4  5       590.30 TB            21.40 PB            9.40 PB        30.97 PB  \n"
     ]
    }
   ],
   "source": [
    "print_report(all_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
