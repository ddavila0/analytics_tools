{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myUtils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that putting together all the dataset ids on every day of a week are the same as the\n",
    "# dataset ids of the week\n",
    "def validate_days_in_weeks(week_ts, weeks_ds, days_ds):\n",
    "    \n",
    "    ret_val = True\n",
    "    # Get the set of dataset_ids accessed in the week identified by 'week_ts'\n",
    "    week_set = set(weeks_ds[weeks_ds['week_ts']==week_ts].datasets_set.values[0])    \n",
    "    \n",
    "    # Get the set of dataset_ids accessed in every day that belongs to a week identified by 'week_ts'\n",
    "    days_set=set()\n",
    "    for day_set in days_ds[days_ds['week_ts']==week_ts]['datasets_set']:\n",
    "        days_set.update(set(day_set))\n",
    "    \n",
    "    # Makes sure both sets are the same size\n",
    "    week_set_len= len(week_set)\n",
    "    days_set_len= len(days_set)\n",
    "    if week_set_len != days_set_len:\n",
    "        print(str(week_set_len)+\" != \"+str(days_set_len))\n",
    "        ret_val = False\n",
    "    \n",
    "    # If both sets are the same size proceed to make sure tha both\n",
    "    # sets contain the same items\n",
    "    if ret_val != False:\n",
    "        if days_set != week_set:\n",
    "            ret_val = False\n",
    "    \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.13 PB\n",
      "997.96 TB\n",
      "2.20 TB\n",
      "2.78 PB\n",
      "1.36 PB\n",
      "5134743402851260.0\n",
      "5134743402851260.0\n"
     ]
    }
   ],
   "source": [
    "# the code below validates that the spliting of job records into different data tier classes\n",
    "# is correct by comparing the size of the sizes of the set of datasets accessed in a specific day for:\n",
    "# 1. The DataFrame where all datatiers are together and\n",
    "# 2. The DataFrame where datatiers asre separated in 4 classes (0 to 3)\n",
    "\n",
    "datasets_df = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/dataset.parquet/\")\n",
    "datasets_size = datasets_df[['d_dataset_id', 'dataset_size']]\n",
    "# Read DatafRame where job records are separated in 4 datatier classes\n",
    "datasets_tiers = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/data_tier_days_201906.parquet/\")\n",
    "\n",
    "# Read DatafRame where jobs from different datatiers are put together\n",
    "datasets_all = pd.read_parquet(\"/Users/ddavila/projects/DOMA/data/model/days_201906.parquet/\")\n",
    "\n",
    "# Claculate the size of the DataFrame with no datatier distinction\n",
    "all_set=set()\n",
    "for i in datasets_all[datasets_all['day_ts']==1559779200.0].datasets_set.values[0]:\n",
    "    all_set.add(i)\n",
    "all_size = get_size_of_datasets_set(all_set, datasets_size)    \n",
    "print(format_bytes(all_size))\n",
    "\n",
    "# Calculate the size of each datatier class\n",
    "tier_set0=set()\n",
    "a = datasets_tiers['data_data_tier_name_class']==0\n",
    "b = datasets_tiers['day_ts']==1559779200.0\n",
    "for j in datasets_tiers[a&b].datasets_set.values[0]:\n",
    "    tier_set0.add(j)\n",
    "tier0_size = get_size_of_datasets_set(tier_set0, datasets_size)\n",
    "print(format_bytes(tier0_size))\n",
    "\n",
    "tier_set1=set()\n",
    "a = datasets_tiers['data_data_tier_name_class']==1\n",
    "b = datasets_tiers['day_ts']==1559779200.0\n",
    "for j in datasets_tiers[a&b].datasets_set.values[0]:\n",
    "    tier_set1.add(j)\n",
    "tier1_size = get_size_of_datasets_set(tier_set1, datasets_size)\n",
    "print(format_bytes(tier1_size))\n",
    "\n",
    "tier_set2=set()\n",
    "a = datasets_tiers['data_data_tier_name_class']==2\n",
    "b = datasets_tiers['day_ts']==1559779200.0\n",
    "for j in datasets_tiers[a&b].datasets_set.values[0]:\n",
    "    tier_set2.add(j)\n",
    "tier2_size = get_size_of_datasets_set(tier_set2, datasets_size)\n",
    "print(format_bytes(tier2_size))\n",
    "\n",
    "tier_set3=set()\n",
    "a = datasets_tiers['data_data_tier_name_class']==3\n",
    "b = datasets_tiers['day_ts']==1559779200.0\n",
    "for j in datasets_tiers[a&b].datasets_set.values[0]:\n",
    "    tier_set3.add(j)\n",
    "tier3_size = get_size_of_datasets_set(tier_set3, datasets_size)\n",
    "print(format_bytes(tier3_size))\n",
    "\n",
    "# The sum of the sizes of the different classes is the same as the\n",
    "# size of all the classes put together\n",
    "print(tier0_size + tier1_size +tier2_size +tier3_size)\n",
    "print(all_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
